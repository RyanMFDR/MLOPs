{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\arion syemael\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4) (4.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.30.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\arion syemael\\appdata\\roaming\\python\\python311\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\arion syemael\\appdata\\roaming\\python\\python311\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\arion syemael\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping selesai! Data disimpan ke jptiik_articles.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import re  # Digunakan untuk mencari ID yang diawali \"article-\"\n",
    "\n",
    "def scrape_jptiik(max_articles=5, output_file=\"jptiik_articles.csv\"):\n",
    "    url = 'https://j-ptiik.ub.ac.id/index.php/j-ptiik/'\n",
    "\n",
    "    # Konfigurasi Selenium\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Jalankan tanpa membuka browser\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")  # Hindari deteksi bot\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Tunggu agar JavaScript dapat memuat halaman sepenuhnya\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # Cari semua artikel yang ada di daftar\n",
    "    articles = soup.find_all('div', class_='obj_article_summary')\n",
    "\n",
    "    # Simpan hasil ke dalam list\n",
    "    scraped_data = []\n",
    "\n",
    "    count = 0  # Untuk menghitung berapa artikel yang sudah diambil\n",
    "\n",
    "    for article in articles:\n",
    "        if count >= max_articles:\n",
    "            break  # Berhenti jika sudah mencapai jumlah yang diinginkan\n",
    "\n",
    "        # Ambil judul artikel (dari <a> yang memiliki id diawali \"article-\")\n",
    "        title_tag = article.find('a', id=re.compile(r'^article-\\d+'))  # Cari <a> dengan id \"article-XXXX\"\n",
    "        title = title_tag.text.strip() if title_tag else 'Tidak ditemukan'\n",
    "\n",
    "        # Ambil nama penulis\n",
    "        author_tag = article.find('div', class_='authors')\n",
    "        author = author_tag.text.strip() if author_tag else 'Tidak ditemukan'\n",
    "\n",
    "        scraped_data.append([title, author])  # Simpan ke dalam list\n",
    "\n",
    "        count += 1  # Tambah counter setelah mengambil satu artikel\n",
    "\n",
    "    # Simpan ke file CSV\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Judul\", \"Penulis\"])  # Header CSV\n",
    "        writer.writerows(scraped_data)  # Data\n",
    "\n",
    "    print(f\"Scraping selesai! Data disimpan ke {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Masukkan jumlah artikel yang ingin diambil\n",
    "    scrape_jptiik(max_articles=2000, output_file=\"jptiik_articles.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Request error: HTTPSConnectionPool(host='j-ptiik.ub.ac.id', port=443): Max retries exceeded with url: /index.php/j-ptiik/issue/archive (Caused by ProxyError('Unable to connect to proxy', NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001ED7680E390>: Failed to resolve 'your_proxy_here' ([Errno 11001] getaddrinfo failed)\")))\n",
      "✅ Semua data disimpan di jurnal_ktiik.csv\n",
      "✅ Scraping selesai!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Konfigurasi header agar tidak terdeteksi sebagai bot\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://j-ptiik.ub.ac.id/index.php/j-ptiik/issue/archive\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "# Masukkan cookies dari browser jika perlu (dapatkan dari browser dengan EditThisCookie)\n",
    "COOKIES = {\n",
    "    \"PHPSESSID\": \"your_session_id_here\",\n",
    "    \"other_cookie\": \"value_here\"\n",
    "}\n",
    "\n",
    "# Gunakan proxy jika diperlukan\n",
    "PROXIES = {\n",
    "    \"http\": \"http://your_proxy_here\",\n",
    "    \"https\": \"https://your_proxy_here\"\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://j-ptiik.ub.ac.id\"\n",
    "ARCHIVE_URL = \"https://j-ptiik.ub.ac.id/index.php/j-ptiik/issue/archive\"\n",
    "OUTPUT_CSV = \"jurnal_ktiik.csv\"\n",
    "\n",
    "# Fungsi untuk mengambil halaman menggunakan requests (fallback ke Selenium jika gagal)\n",
    "def get_html(url, use_selenium=False):\n",
    "    \"\"\"Mengambil HTML dari halaman, dengan opsi menggunakan Selenium jika requests gagal.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, cookies=COOKIES, proxies=PROXIES, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"⚠️ 403 Forbidden: Menggunakan Selenium untuk {url}\")\n",
    "            return get_html_selenium(url)\n",
    "        else:\n",
    "            print(f\"❌ Gagal mengambil halaman {url}: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌ Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Fungsi untuk mengambil HTML menggunakan Selenium\n",
    "def get_html_selenium(url):\n",
    "    \"\"\"Menggunakan Selenium untuk mendapatkan HTML dari halaman yang diproteksi.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-maximized\")  # Tampilkan browser\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Tunggu agar halaman benar-benar termuat\n",
    "    \n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "    return html\n",
    "\n",
    "# Fungsi mengambil link edisi jurnal (bulan penerbitan)\n",
    "def get_monthly_links():\n",
    "    \"\"\"Mengambil semua link bulan penerbitan dari halaman arsip.\"\"\"\n",
    "    html = get_html(ARCHIVE_URL)\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    monthly_links = [(month.text.strip(), BASE_URL + month['href']) for month in soup.find_all('a', class_='title')]\n",
    "    return monthly_links\n",
    "\n",
    "# Fungsi mengambil link artikel dalam satu edisi jurnal\n",
    "def get_article_links(issue_url):\n",
    "    \"\"\"Mengambil semua link artikel dari halaman edisi jurnal.\"\"\"\n",
    "    html = get_html(issue_url)\n",
    "    if not html:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    articles = soup.find_all('a', id=re.compile(r'^article-\\d+'))\n",
    "    article_links = [(article.text.strip(), BASE_URL + article['href']) for article in articles]\n",
    "    \n",
    "    return article_links\n",
    "\n",
    "# Fungsi mengambil detail artikel (judul & author)\n",
    "def scrape_article_details(article_url):\n",
    "    \"\"\"Scraping detail artikel (judul dan penulis) menggunakan Selenium jika perlu.\"\"\"\n",
    "    html = get_html(article_url, use_selenium=True)\n",
    "    if not html:\n",
    "        return \"Tidak ditemukan\", \"Tidak ditemukan\"\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_='page_title')  # Coba cari elemen judul\n",
    "    title = title_tag.text.strip() if title_tag else \"Tidak ditemukan\"\n",
    "\n",
    "    author_tag = soup.find('div', class_='authors')  # Cari elemen author\n",
    "    authors = author_tag.text.strip() if author_tag else \"Tidak ditemukan\"\n",
    "\n",
    "    return title, authors\n",
    "\n",
    "# Fungsi utama scraping semua artikel\n",
    "def scrape_articles():\n",
    "    \"\"\"Scraping semua artikel dari semua bulan penerbitan dan simpan dalam satu CSV.\"\"\"\n",
    "    monthly_links = get_monthly_links()\n",
    "    all_data = []\n",
    "\n",
    "    for month_name, month_url in monthly_links:\n",
    "        print(f\"🔍 Scraping bulan: {month_name}\")\n",
    "        article_links = get_article_links(month_url)\n",
    "\n",
    "        for article_title, article_url in article_links:\n",
    "            title, authors = scrape_article_details(article_url)\n",
    "            all_data.append([month_name, title, authors])\n",
    "            print(f\"✅ {title} - {authors}\")\n",
    "            time.sleep(1)  # Hindari pemblokiran dengan delay\n",
    "\n",
    "    save_to_csv(all_data)\n",
    "\n",
    "# Fungsi menyimpan hasil scraping ke CSV\n",
    "def save_to_csv(data):\n",
    "    \"\"\"Simpan hasil scraping ke dalam satu file CSV.\"\"\"\n",
    "    with open(OUTPUT_CSV, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Bulan\", \"Judul\", \"Penulis\"])\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"✅ Semua data disimpan di {OUTPUT_CSV}\")\n",
    "\n",
    "# Eksekusi kode utama\n",
    "if __name__ == '__main__':\n",
    "    scrape_articles()\n",
    "    print(\"✅ Scraping selesai!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
